# Visual-Question-Answering-In-Medical-Domain
VQA is a multidisciplinary problem which combines two modalities: text and image. It
requires computer vision and NLP techniques (probably, reasoning techniques too). The
task is to answer a question correctly, where the question is accompanied by an image
on which it is based.

This assignment is a domain specific challenge. ImageCLEF defines the challenge as
follows: “Given a medical image accompanied with a clinically relevant question,
participating systems are tasked with answering the question based on the visual image
content”.

Please refer to the image below for Baseline Model Architecture:
![alt text](https://github.com/meghanakotagiri/Visual-Question-Answering-In-Medical-Domain/blob/master/AMP%20ASS%202.jpg)

For more details related to implementation and results of the project, please refer to file [AMP REPORT.pdf](https://github.com/meghanakotagiri/Visual-Question-Answering-In-Medical-Domain/blob/master/AMP%20REPORT.pdf)
and for details regarding code, pease refer to file [ReadMe.txt]  (https://github.com/meghanakotagiri/Visual-Question-Answering-In-Medical-Domain/edit/master/code/ReadMe.txt) in Code Folder.
